{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ph21: HWK 1\n",
    "In this assignment, we will get Buoy data from the NDBC and then use it to make some estimates about the future.\n",
    "\n",
    "### Familiarize yourself\n",
    "\n",
    "We will be using data from the National Data Buoy Center (NDBC). Visit the NDBC website, https://www.ndbc.noaa.gov, and access some historical data. Take a look at the URL for the data you have accessed: you will need this format to access the data from python.\n",
    "\n",
    "### Part 1: Getting the data\n",
    "\n",
    "You will need to save all necessary for your second part here. The following steps are for reference.\n",
    "\n",
    "1. Use the `requests` library (or any other similar library at your choice) to download historical Buoy data\n",
    "3. Save all data you downloaded into (a) file(s). If you feel like it, only save that part that is relevant for your second part.\n",
    "4. Use a html parser library (e.g., `BeautifulSoup`, `lxml`, or `html5lib`) to extract the meaning of the data (i.e. what is it measuring and what are the units?). Also save this information somewhere.\n",
    "5. From now on, you don't need to make any more requests to the NDBC website. Only use the data you downloaded.\n",
    "\n",
    "\n",
    "### Part 2: Processing the data\n",
    "6. (Optional) use the `pandas` library to read the data and process as needed, and save the processed data if you want to. Save the processed data\n",
    "7. plot the oceant temperatures (`WTMP`), wave heights (`WVHT`), average wave periods (`APD`), and wind speeds (`WSPD`) going back 10 years or so (some buoys don't have all the data every year)\n",
    "8. Look through `scipy.stats` and choose something like Pearson's or Spearman's correlation test. Determine what (if any) correlations you find between mean ocean temperatures and maximum wave heights or wave periods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All written codes are only here for reference. You can also modify them if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries you might want to use\n",
    "# Uncomment the ones you need\n",
    "# from datetime import datetime\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "from requests import Session\n",
    "# from curl_cffi.requests import Session, AsyncSession\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# from lxml import html\n",
    "\n",
    "# from matplotlib import pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# from scipy import stats\n",
    "\n",
    "import yaml\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare requests session. Let's pretend we are Chrome 118 here\n",
    "\n",
    "# Think: In some cases, we can still be blocked as been flagged as bot at the first request.\n",
    "#        Can you guess how the server knows it? (ignore javascript for now)\n",
    "\n",
    "sess = Session()\n",
    "sess.headers.update({'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36'})\n",
    "\n",
    "# Think: What's the difference between using requests.get() and sess.get()?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the fetched data\n",
    "path = Path('./Data')\n",
    "path.mkdir(exist_ok=True)\n",
    "url_template = 'https://www.ndbc.noaa.gov/view_text_file.php?filename={buoy_id}h{year}.txt.gz&dir=data/historical/stdmet/'\n",
    "description_page_url = 'https://www.ndbc.noaa.gov/faq/measdes.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the config file\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "buoy_ids:list[int] = config['buoys'] # We only use these id's for this assignment\n",
    "columns:list[str] = config['columns'] # Data columns we want to keep. You may add more if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fetch one file, you can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to fetch the description and put it in a pd.DataFrame\n",
    "\n",
    "r = sess.get(url_template.format(buoy_id=41010, year=2022))\n",
    "r.raise_for_status()\n",
    "buf= StringIO(r.text)\n",
    "df = pd.read_csv(buf, delim_whitespace=True, skiprows=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task here is:\n",
    "1. For each buoy_id, generate all urls pointing to its data, and fetch&save the data. If the data is missing (`status_code`=404), skip it.\n",
    "2. Save everything in raw, or any processed form that you see fits.\n",
    "3. If the file is already downloaded, don't download it again. Just skip it here.\n",
    "\n",
    "Learn more about http status codes here:\n",
    "- https://en.wikipedia.org/wiki/List_of_HTTP_status_codes\n",
    "- https://http.cat/\n",
    "\n",
    "Two important ones here for today:\n",
    "- 200: OK; everything's alright\n",
    "- 404: Not Found; the file is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ INSERT YOUR CODE HERE ================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the units for all quantities are included in the data files received already, lets also try get them again from the website along with the description for each columns by parsing htmls.\n",
    "\n",
    "All information of our interest is available at [`description_page`](https://www.ndbc.noaa.gov/faq/measdes.shtml#stdmet).\n",
    "\n",
    "Your task is to:\n",
    "\n",
    "1. Get a dictionary of all the quantity name and its unit\n",
    "2. Get a dictionary of all the quantity name and its description\n",
    "\n",
    "To achieve this, you may want to use some html parser library, such as `BeautifulSoup`, `lxml`, or `html5lib`. If you really feel like it, you can also use regex just for fun like the good old days.\n",
    "\n",
    "Tip: Having no idea where to start? Try to open the [`description_page`](https://www.ndbc.noaa.gov/faq/measdes.shtml#stdmet) in your browser, and use the `inspect` tool (F12, or Ctrl+Shift+I) to see how the html is structured and locate the part of your interest. Pasting a copy of the html to some LLM and ask it how to extract the information you want may also be a good idea.\n",
    "\n",
    "Another Tip: Something called [`xpath`](https://en.wikipedia.org/wiki/XPath) may be extremely useful here. It can be access with `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sess.get(description_page_url)\n",
    "r.raise_for_status()\n",
    "html_text = r.text # raw html text for your input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ INSERT YOUR CODE HERE ================\n",
    "\n",
    "if not (path/'units.yaml').exists() or not (path/'meanings.yaml').exists():\n",
    "    ...\n",
    "\n",
    "    with open(path/'units.yaml', 'w') as f:\n",
    "        yaml.dump(unit_map, f)\n",
    "    with open(path/'meanings.yaml', 'w') as f:\n",
    "        yaml.dump(meaning_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path/'units.yaml') as f:\n",
    "    unit_map = yaml.safe_load(f)\n",
    "unit_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path/'meanings.yaml') as f:\n",
    "    meaning_map = yaml.safe_load(f)\n",
    "meaning_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to concatenate each buoy's data into a single dataframe, or any other data structure that you see fit. `pandas` is recommended here for your convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting, you will notice that some values are saturated at ~100. This is considered as artifacts, and should be removed. We will filter out everything $\\ge 90$ in this case. If you are using `pandas`, you can do something like `df[df[columns] < 90]`.\n",
    "Also, remove all rows with `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ INSERT YOUR CODE HERE ================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As python handles datetime objects in fundamentally nasty way, we provide a snippet to convert YMDhm to datetime objects for you. You can use it like this:\n",
    "\n",
    "```python\n",
    "YMDhm = df[['#YY','MM','DD','hh','mm']]\n",
    "timestemp = YMDhm.agg(lambda x: datetime(*x), axis=1)\n",
    "```\n",
    "\n",
    "to get a series of datetime objects from the dataframe `df` with columns `#YY`, `MM`, `DD`, `hh`, and `mm`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For one buoy, plot all of its columns from all last 10 years. You can use any plotter of your choice.\n",
    "##### Can you see any trend? If it is too noisy for human eyes, what could to be done to make it more clear?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Launch `scipy.stats` and choose something like Pearson's or Spearman's correlation test. Determine what (if any) correlations you find between mean ocean temperatures and maximum wave heights or wave periods.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================ INSERT YOUR CODE HERE ================\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
